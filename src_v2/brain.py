# Module Brain (Cerveau)
# G√®re le chargement du mod√®le LLM et la g√©n√©ration de r√©ponses.

import sys
from llama_cpp import Llama
from huggingface_hub import hf_hub_download
import skills  # Import du module de comp√©tences

class Brain:
    def __init__(self, model_repo="TheBloke/Mistral-7B-Instruct-v0.2-GGUF", model_file="mistral-7b-instruct-v0.2.Q4_K_M.gguf"):
        self.model_path = self._download_model(model_repo, model_file)
        print(f"üß† Chargement du mod√®le depuis {self.model_path}...")
        
        # Configuration pour GPU (n_gpu_layers=-1 pour tout mettre sur le GPU)
        # verbose=False pour √©viter de spammer la console
        try:
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=2048,          # Contexte r√©duit pour √©viter OOM (4096 -> 2048)
                n_gpu_layers=-1,     # Utilise tout le GPU disponible
                verbose=False
            )
            print("‚úÖ Cerveau charg√© (Mode GPU activ√©) !")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur chargement GPU, tentative CPU... ({e})")
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=2048,
                n_gpu_layers=0,      # Mode CPU
                verbose=False
            )
            print("‚úÖ Cerveau charg√© (Mode CPU secours) !")

        # D√©finition de la personnalit√© et des outils
        self.system_prompt = """Tu es Jarvis, une IA assistante intelligente et utile connect√©e √† un PC Windows.
Tu dois r√©pondre de mani√®re concise et pr√©cise en fran√ßais.
Tu as la capacit√© d'effectuer des actions r√©elles sur l'ordinateur.

COMMANDES DISPONIBLES :
- Pour ouvrir Google Chrome, r√©ponds UNIQUEMENT : [CMD:open_chrome]
- Pour ouvrir YouTube, r√©ponds UNIQUEMENT : [CMD:open_youtube]

R√àGLES :
1. Si l'utilisateur demande une action list√©e ci-dessus, utilise le code [CMD:...] correspondant.
2. Sinon, r√©ponds normalement √† la question.
3. Ne dis jamais "Je ne peux pas faire √ßa" pour les actions list√©es ci-dessus.
"""

    def _download_model(self, repo_id, filename):
        """T√©l√©charge le mod√®le automatiquement si absent."""
        print(f"üîç V√©rification du mod√®le {filename}...")
        try:
            model_path = hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                local_dir="./models",
                local_dir_use_symlinks=False
            )
            return model_path
        except Exception as e:
            print(f"‚ùå Erreur de t√©l√©chargement : {e}")
            sys.exit(1)

    def think(self, text):
        """G√©n√®re une r√©ponse √† partir du texte utilisateur."""
        # Construction du prompt avec System Prompt (Format Mistral)
        # On injecte le system prompt avant l'instruction utilisateur
        full_prompt = f"[INST] {self.system_prompt}\n\nUtilisateur : {text} [/INST]"
        
        output = self.llm(
            full_prompt,
            max_tokens=512,
            stop=["</s>", "[/INST]"],
            echo=False
        )
        
        response = output['choices'][0]['text'].strip()
        
        # D√©tection et ex√©cution des commandes
        if "[CMD:open_chrome]" in response:
            skills.execute_skill("open_chrome")
            return "J'ouvre Google Chrome pour vous."
        elif "[CMD:open_youtube]" in response:
            skills.execute_skill("open_youtube")
            return "J'ouvre YouTube tout de suite."
            
        return response

    def analyze_intent(self, text):
        """
        Analyse l'intention avec le LLM si les regex ont √©chou√©.
        Retourne un dict {intent, parameters}.
        """
        import json
        import re
        
        prompt = f"""[INST] Tu es un analyseur d'intentions pour un assistant vocal.
Ton but est de convertir une phrase utilisateur en commande structur√©e JSON.

INTENTIONS POSSIBLES :
- web_search : pour faire une recherche sur internet (param: query)
- open_app : pour ouvrir un logiciel (param: app_name)
- close_app : pour fermer un logiciel (param: app_name)
- scroll_down : pour descendre dans une page
- scroll_up : pour monter dans une page
- small_talk : pour la conversation (param: type=greeting|thanks|goodbye|unknown)
- unknown : si aucune intention ne correspond

R√àGLES :
1. R√©ponds UNIQUEMENT avec le JSON valide.
2. Pas de blabla avant ou apr√®s.
3. Si c'est ambigu, choisis l'intention la plus probable.

Exemples :
"Cherche m√©t√©o Paris" -> {{"intent": "web_search", "parameters": {{"query": "m√©t√©o Paris"}}}}
"Ouvre Chrome" -> {{"intent": "open_app", "parameters": {{"app_name": "Chrome"}}}}
"Descends un peu" -> {{"intent": "scroll_down", "parameters": {{}}}}
"Bonjour" -> {{"intent": "small_talk", "parameters": {{"type": "greeting"}}}}

Phrase √† analyser : "{text}" [/INST]"""

        try:
            output = self.llm(
                prompt,
                max_tokens=128,
                stop=["</s>", "[/INST]"],
                echo=False,
                temperature=0.1 # Tr√®s d√©terministe
            )
            
            response = output['choices'][0]['text'].strip()
            
            # Nettoyage pour extraire le JSON (au cas o√π le LLM bavarde)
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            else:
                return {'intent': 'unknown', 'parameters': {}}
                
        except Exception as e:
            print(f"Erreur analyse LLM : {e}")
            return {'intent': 'unknown', 'parameters': {}}

# Test rapide si ex√©cut√© directement
if __name__ == "__main__":
    brain = Brain()
    print("R√©ponse :", brain.think("Ouvre chrome s'il te plait"))

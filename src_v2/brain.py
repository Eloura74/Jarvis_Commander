# Module Brain (Cerveau)
# G√®re le chargement du mod√®le LLM et la g√©n√©ration de r√©ponses.

import sys
from llama_cpp import Llama
from huggingface_hub import hf_hub_download
import skills  # Import du module de comp√©tences

class Brain:
    def __init__(self, model_repo="TheBloke/Mistral-7B-Instruct-v0.2-GGUF", model_file="mistral-7b-instruct-v0.2.Q4_K_M.gguf"):
        self.model_path = self._download_model(model_repo, model_file)
        print(f"üß† Chargement du mod√®le depuis {self.model_path}...")
        
        # Configuration pour GPU (n_gpu_layers=-1 pour tout mettre sur le GPU)
        # verbose=False pour √©viter de spammer la console
        try:
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=2048,          # Contexte r√©duit pour √©viter OOM (4096 -> 2048)
                n_gpu_layers=-1,     # Utilise tout le GPU disponible
                verbose=False
            )
            print("‚úÖ Cerveau charg√© (Mode GPU activ√©) !")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur chargement GPU, tentative CPU... ({e})")
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=2048,
                n_gpu_layers=0,      # Mode CPU
                verbose=False
            )
            print("‚úÖ Cerveau charg√© (Mode CPU secours) !")

        # D√©finition de la personnalit√© et des outils
        self.system_prompt = """Tu es Jarvis, une IA assistante intelligente et utile connect√©e √† un PC Windows.
Tu dois r√©pondre de mani√®re concise et pr√©cise en fran√ßais.
Tu as la capacit√© d'effectuer des actions r√©elles sur l'ordinateur.

COMMANDES DISPONIBLES :
- Pour ouvrir Google Chrome, r√©ponds UNIQUEMENT : [CMD:open_chrome]
- Pour ouvrir YouTube, r√©ponds UNIQUEMENT : [CMD:open_youtube]

R√àGLES :
1. Si l'utilisateur demande une action list√©e ci-dessus, utilise le code [CMD:...] correspondant.
2. Sinon, r√©ponds normalement √† la question.
3. Ne dis jamais "Je ne peux pas faire √ßa" pour les actions list√©es ci-dessus.
"""

    def _download_model(self, repo_id, filename):
        """T√©l√©charge le mod√®le automatiquement si absent."""
        print(f"üîç V√©rification du mod√®le {filename}...")
        try:
            model_path = hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                local_dir="./models",
                local_dir_use_symlinks=False
            )
            return model_path
        except Exception as e:
            print(f"‚ùå Erreur de t√©l√©chargement : {e}")
            sys.exit(1)

    def think(self, text):
        """G√©n√®re une r√©ponse √† partir du texte utilisateur."""
        # Construction du prompt avec System Prompt (Format Mistral)
        # On injecte le system prompt avant l'instruction utilisateur
        full_prompt = f"[INST] {self.system_prompt}\n\nUtilisateur : {text} [/INST]"
        
        output = self.llm(
            full_prompt,
            max_tokens=512,
            stop=["</s>", "[/INST]"],
            echo=False
        )
        
        response = output['choices'][0]['text'].strip()
        
        # D√©tection et ex√©cution des commandes
        if "[CMD:open_chrome]" in response:
            skills.execute_skill("open_chrome")
            return "J'ouvre Google Chrome pour vous."
        elif "[CMD:open_youtube]" in response:
            skills.execute_skill("open_youtube")
            return "J'ouvre YouTube tout de suite."
            
        return response

# Test rapide si ex√©cut√© directement
if __name__ == "__main__":
    brain = Brain()
    print("R√©ponse :", brain.think("Ouvre chrome s'il te plait"))
